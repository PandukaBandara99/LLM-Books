{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNgQR6Madq7w61TJPkfz1Vn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PandukaBandara99/LLM-Books/blob/main/LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mahela Panduka Bandara<br>\n",
        "B.Sc. Eng. (Hons) in Electrical and Electronic Engineering, University of Peradeniya"
      ],
      "metadata": {
        "id": "YCLF5i3IzURY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a comprehensive workbook covering everything from A to Z about LangChain and LangGraph.\n",
        "\n"
      ],
      "metadata": {
        "id": "C4gERxKbzeoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**LangChain & LangGraph Complete Content List- TB covered**\n",
        "\n",
        "---\n",
        "\n",
        "### **PART 1: Getting Started with LangChain**\n",
        "\n",
        "#### 1. Introduction to LangChain\n",
        "\n",
        "* What is LangChain?\n",
        "* Why use LangChain with LLMs?\n",
        "* Core architecture and modular design\n",
        "* Use cases: QA, agents, chatbots, retrieval-augmented generation (RAG)\n",
        "\n",
        "#### 2. LangChain Installation & Setup\n",
        "\n",
        "* Installing LangChain (`pip install langchain`)\n",
        "* Integrating with OpenAI, HuggingFace, Cohere, etc.\n",
        "* API Keys and Environment setup\n",
        "* Jupyter and Streamlit compatibility\n",
        "\n",
        "#### 3. LangChain Building Blocks\n",
        "\n",
        "* **LLMs**: Wrappers and model invocation\n",
        "* **Prompt Templates**: Input variables, formatting, chaining\n",
        "* **Chains**: SequentialChain, SimpleChain, and more\n",
        "* **Memory**: Short-term & long-term memory, ConversationBufferMemory\n",
        "* **Agents**: Tool usage, planning, decision-making\n",
        "\n",
        "---\n",
        "\n",
        "### **PART 2: LangChain Modules in Depth**\n",
        "\n",
        "#### 4. Prompt Templates\n",
        "\n",
        "* Static vs dynamic prompts\n",
        "* Few-shot prompt templates\n",
        "* Custom prompt design and formatting\n",
        "\n",
        "#### 5. Language Models (LLMs)\n",
        "\n",
        "* OpenAI, Anthropic, Cohere integration\n",
        "* Chat vs completion APIs\n",
        "* Output parsers and structured outputs\n",
        "\n",
        "#### 6. Chains\n",
        "\n",
        "* SimpleChain, LLMChain, SequentialChain\n",
        "* RouterChain and MultiPromptChain\n",
        "* Custom chains and chain callbacks\n",
        "* Loading chains from config (langchain hub)\n",
        "\n",
        "#### 7. Memory Management\n",
        "\n",
        "* ConversationBufferMemory\n",
        "* TokenWindowMemory\n",
        "* VectorStore-backed memory\n",
        "* Chat history and persistence\n",
        "\n",
        "#### 8. Tools and Agents\n",
        "\n",
        "* Defining tools\n",
        "* Built-in tools (Google Search, calculator, Python REPL, etc.)\n",
        "* Zero-shot & ReAct Agents\n",
        "* Custom agent creation\n",
        "\n",
        "#### 9. Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "* Vector stores (FAISS, Chroma, Pinecone, etc.)\n",
        "* Document loaders and splitters\n",
        "* Embeddings\n",
        "* Retrieval with semantic search\n",
        "* Combining retrievers with chains\n",
        "\n",
        "---\n",
        "\n",
        "### **PART 3: LangGraph for Agent Workflow Orchestration**\n",
        "\n",
        "#### 10. Introduction to LangGraph\n",
        "\n",
        "* What is LangGraph?\n",
        "* LangGraph vs LangChain\n",
        "* Building multi-step LLM workflows\n",
        "* Async workflows and graph orchestration\n",
        "\n",
        "#### 11. LangGraph Setup\n",
        "\n",
        "* Installation and dependencies\n",
        "* Architecture: Nodes, Edges, States\n",
        "* Interfacing with LangChain\n",
        "\n",
        "#### 12. Creating LangGraphs\n",
        "\n",
        "* Defining graph steps (nodes)\n",
        "* State management and transitions\n",
        "* Conditional flows and loops\n",
        "* Persistent and resumable flows\n",
        "\n",
        "#### 13. Agentic Workflows with LangGraph\n",
        "\n",
        "* Multi-agent collaboration\n",
        "* Decision nodes and context passing\n",
        "* Modular workflows for RAG + QA + reasoning\n",
        "* Error handling in workflows\n",
        "\n",
        "#### 14. LangGraph Use Cases\n",
        "\n",
        "* Autonomous agents\n",
        "* Customer support flow\n",
        "* Multi-tool orchestration\n",
        "* Retrieval and long-running memory workflows\n",
        "\n",
        "---\n",
        "\n",
        "### **PART 4: Integrations and Applications**\n",
        "\n",
        "#### 15. Tooling and Extensions\n",
        "\n",
        "* LangSmith for tracing/debugging\n",
        "* LangChain Hub for shared chains\n",
        "* OpenAI function calling\n",
        "* Open source vs hosted models\n",
        "\n",
        "#### 16. Frontend and Deployment\n",
        "\n",
        "* Integrating with Streamlit or Gradio\n",
        "* Deploying on FastAPI or Flask\n",
        "* Containerization (Docker)\n",
        "* Serverless options (e.g., Vercel, AWS Lambda)\n",
        "\n",
        "---\n",
        "\n",
        "### **PART 5: Advanced Topics**\n",
        "\n",
        "#### 17. Building Custom Tools and Chains\n",
        "\n",
        "* Writing custom tools (APIs, DB access)\n",
        "* Custom output parsers\n",
        "* Async chains and parallelization\n",
        "\n",
        "#### 18. Security, Cost, and Performance\n",
        "\n",
        "* Rate limiting and throttling\n",
        "* Logging and observability\n",
        "* Token usage and cost management\n",
        "\n",
        "#### 19. Testing and Debugging\n",
        "\n",
        "* Unit testing LangChain components\n",
        "* LangSmith traces and run analytics\n",
        "* Prompt testing and evaluation\n",
        "\n",
        "#### 20. Real-World Projects\n",
        "\n",
        "* End-to-end chatbot with memory and tools\n",
        "* Document Q\\&A with LangGraph flow\n",
        "* Autonomous agent for research/summarization"
      ],
      "metadata": {
        "id": "Q__C13PjzBJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.0 LangChain"
      ],
      "metadata": {
        "id": "_FCxDOg70Lk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Introduction to LangChain"
      ],
      "metadata": {
        "id": "iEjtuVfT0RzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting LLMs effectively in real-world scenarios requires more than just sending prompts and receiving responses. It involves managing memory, chaining tasks, integrating external data, and orchestrating complex flows. This is where **LangChain** emerges as a powerful framework.\n",
        "\n",
        "**What is LangChain?**\n",
        "\n",
        ">**LangChain** is an open-source framework specifically designed to simplify the process of building applications powered by LLMs. It enables developers to chain together various components—LLMs, tools, memory, prompts, retrievers, and agents—into complex yet manageable workflows.\n",
        "\n",
        "> LangChain acts as a middleware between language models and the practical tasks they need to perform. Whether you want to create a simple question-answering system or a complex multi-step agentic workflow, LangChain provides reusable, modular building blocks to do so efficiently.\n",
        "\n",
        "**Why Use LangChain with LLMs?**\n",
        "\n",
        "> While LLMs are powerful, they are inherently stateless and task-agnostic. They don't remember previous messages, can’t fetch data from APIs on their own, and don’t have built-in reasoning loops. LangChain addresses these gaps by introducing\n",
        "\n",
        "- **Chaining Mechanisms**: Connect outputs from one module (like a language model) into the inputs of another. This enables multi-step workflows.\n",
        "- **Memory Components**: Provide context retention between interactions, essential for chatbots and agents.\n",
        "- **Tool Integration**: Use external tools like calculators, APIs, search engines, or file systems.\n",
        "- **Agentic Behavior**: Build agents that can dynamically decide which tools to use, and in what order.\n",
        "\n",
        "This makes LangChain ideal for building production-level applications where LLMs need structure, state, and adaptability.\n",
        "\n",
        "**Core Architecture and Modular Design**\n",
        "\n",
        "LangChain’s design philosophy is centered around **composability and extensibility**. Its architecture consists of several high-level modules, each responsible for a specific piece of functionality\n",
        "\n",
        "* **LLMs and Prompts**: Interfaces for managing prompt templates and sending queries to LLMs.\n",
        "* **Chains**: Sequences of calls where the output of one step feeds into the next. Simple chains (like LLMChain) and complex chains (like SequentialChain or RouterChain) are supported.\n",
        "* **Agents**: Dynamic systems that use reasoning to select tools and actions in real-time.\n",
        "* **Memory**: Modules that store previous interactions, essential for building conversational or contextual systems.\n",
        "* **Retrievers and Vector Stores**: Components for integrating document search and retrieval (especially used in RAG).\n",
        "* **Callbacks and Tracing**: Built-in observability to trace and debug pipelines in real time.\n",
        "\n",
        "This modularity makes LangChain highly customizable and easy to plug into various backends or third-party services.\n",
        "\n",
        "**Use Cases**\n",
        "\n",
        "1. **Question Answering (QA) Systems**\n",
        "\n",
        "    > By integrating retrievers (like FAISS, Pinecone) and document loaders, LangChain enables context-aware QA systems. These systems retrieve relevant documents and then use LLMs to generate precise answers.\n",
        "\n",
        "2. **Agents and Tool-Using Bots**\n",
        "\n",
        "    > LangChain agents can use tools such as search engines, calculators, APIs, or custom scripts. This allows for applications like AI personal assistants, coding agents, or data analysts.\n",
        "\n",
        "3. **Chatbots with Memory**\n",
        "\n",
        "    > Chatbots built using LangChain can maintain state and context over multiple turns. They remember user preferences, summarize past conversations, and provide coherent interactions.\n",
        "\n",
        "4. **Retrieval-Augmented Generation (RAG)**\n",
        "\n",
        "    > LangChain supports RAG pipelines where LLMs are augmented with external knowledge via retrieval. This improves answer accuracy and allows models to \"know\" beyond their training cutoff."
      ],
      "metadata": {
        "id": "S561kPbF2GRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 LangChain Installation & Setup"
      ],
      "metadata": {
        "id": "pGcQF8458KNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing LangChain**\n",
        "\n",
        "LangChain is a Python package that can be installed using pip:\n",
        "\n",
        "```\n",
        "pip install langchain\n",
        "```\n",
        "\n",
        "```\n",
        "# Common extras\n",
        "pip install langchain[openai,cohere,huggingface,chromadb]\n",
        "```\n",
        "**Integrating with LLM Providers**\n",
        "\n",
        "LangChain supports multiple LLM backends. Each provider requires specific setup, typically in the form of API keys.\n",
        "\n",
        "**1. OpenAI**\n",
        "\n",
        "To use OpenAI models (like GPT-4 or GPT-3.5), install the `openai` library:\n",
        "\n",
        "```\n",
        "pip install openai\n",
        "```\n",
        "\n",
        "Then, set your API key:\n",
        "\n",
        "```\n",
        "export OPENAI_API_KEY=\"your-openai-api-key\"\n",
        "```\n",
        "\n",
        "Or use `.env` file with `python-dotenv`:\n",
        "\n",
        "```\n",
        "OPENAI_API_KEY=your-openai-api-key\n",
        "```\n",
        "\n",
        "**2. Hugging Face (Transformers API)**\n",
        "\n",
        "To use models hosted by Hugging Face:\n",
        "\n",
        "```\n",
        "pip install transformers\n",
        "```\n",
        "\n",
        "Then set your API key:\n",
        "\n",
        "```\n",
        "export HUGGINGFACEHUB_API_TOKEN=\"your-hf-token\"\n",
        "```\n",
        "\n",
        "**3. Cohere**\n",
        "\n",
        "Install the Cohere Python SDK:\n",
        "\n",
        "```\n",
        "pip install cohere\n",
        "```\n",
        "\n",
        "Set your key:\n",
        "\n",
        "```\n",
        "export COHERE_API_KEY=\"your-cohere-api-key\"\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**API Keys and Environment Setup**\n",
        "\n",
        "LangChain encourages the use of `.env` files for secure environment management. Here’s how to configure one\n",
        "\n",
        "1. Create a file called `.env` in your project root.\n",
        "2. Add your keys\n",
        "```\n",
        "OPENAI_API_KEY=sk-...\n",
        "HUGGINGFACEHUB_API_TOKEN=hf_...\n",
        "COHERE_API_KEY=...\n",
        "```\n",
        "\n",
        "3. Load it using Python:\n",
        "```\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "```\n",
        "\n",
        "**Jupyter and Streamlit Compatibility**\n",
        "\n",
        "LangChain works smoothly in both **Jupyter Notebooks** and **Streamlit** apps, giving you flexibility in development and deployment.\n",
        "\n",
        "### **Jupyter Notebook**\n",
        "\n",
        "Ideal for exploration, debugging, and tutorials. Simply start a notebook:\n",
        "\n",
        "```\n",
        "jupyter notebook\n",
        "```\n",
        "\n",
        "You can prototype LangChain chains, agents, and tools interactively.\n",
        "\n",
        "### **Streamlit**\n",
        "\n",
        "Perfect for building interactive front-end apps powered by LangChain\n",
        "\n",
        "```bash\n",
        "pip install streamlit\n",
        "```\n",
        "\n",
        "Here’s a basic Streamlit template:\n",
        "\n",
        "```python\n",
        "import streamlit as st\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "st.title(\"LangChain App\")\n",
        "\n",
        "prompt = st.text_input(\"Enter your prompt:\")\n",
        "\n",
        "if prompt:\n",
        "    llm = OpenAI(temperature=0.7)\n",
        "    response = llm(prompt)\n",
        "    st.write(response)\n",
        "```\n",
        "\n",
        "Run your app:\n",
        "\n",
        "```bash\n",
        "streamlit run app.py\n",
        "```"
      ],
      "metadata": {
        "id": "DCrXyDOx7_pY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u1VoWS0q0Zu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DcAIeprxBhm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M720PA-_xFvj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}